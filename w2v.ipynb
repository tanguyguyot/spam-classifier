{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b782c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 23:33:37.812220: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-18 23:33:38.432366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-18 23:33:41.786100: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/tguyot/PersonalCode/spam-classifier/.venv/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tguyot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unidecode\n",
    "import re\n",
    "from spacy.cli import download\n",
    "import spacy\n",
    "import nltk\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a058b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_spam</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ok lar joking wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>dun say early hor c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>nah I think go usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>0</td>\n",
       "      <td>nd time try contact win aPS pound prize claim ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5557</th>\n",
       "      <td>1</td>\n",
       "      <td>I b go esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5558</th>\n",
       "      <td>1</td>\n",
       "      <td>pity mood soany suggestion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5559</th>\n",
       "      <td>1</td>\n",
       "      <td>guy bitching I act like I interested buy somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>1</td>\n",
       "      <td>rofl true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5561 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      is_spam                                                sms\n",
       "0           1  go jurong point crazy available bugis n great ...\n",
       "1           1                              ok lar joking wif oni\n",
       "2           0  free entry wkly comp win fa cup final tkts st ...\n",
       "3           1                    dun say early hor c already say\n",
       "4           1              nah I think go usf live around though\n",
       "...       ...                                                ...\n",
       "5556        0  nd time try contact win aPS pound prize claim ...\n",
       "5557        1                           I b go esplanade fr home\n",
       "5558        1                         pity mood soany suggestion\n",
       "5559        1  guy bitching I act like I interested buy somet...\n",
       "5560        1                                     rofl true name\n",
       "\n",
       "[5561 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import cleaned preprocess csv\n",
    "df = pd.read_csv('cleaned_spam.csv').drop(columns=['Unnamed: 0'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed70e6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'bugis',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_corpus(df):\n",
    "    return ' '.join(' '.join(df.sms.tolist()).split())\n",
    "\n",
    "def get_unique_words(df):\n",
    "    corpus = get_corpus(df)\n",
    "    unique = set()\n",
    "    output = []\n",
    "    for word in corpus.split():\n",
    "        if word not in unique:\n",
    "            unique.add(word)\n",
    "            output.append(word)\n",
    "    return output\n",
    "\n",
    "def count_unique(df):\n",
    "    return len(get_unique_words(df))\n",
    "\n",
    "\n",
    "\n",
    "count_unique(df)\n",
    "\n",
    "get_unique_words(df)[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6526975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 0.9998831152915955),\n",
       " ('free', 0.9998794198036194),\n",
       " ('txt', 0.999877393245697),\n",
       " ('reply', 0.9998714327812195),\n",
       " ('get', 0.999868631362915),\n",
       " ('send', 0.999865710735321),\n",
       " ('message', 0.9998558759689331),\n",
       " ('take', 0.9998549818992615),\n",
       " ('go', 0.9998549818992615),\n",
       " ('call', 0.999854564666748)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with gensim first\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['sms'].apply(lambda x: x.split()),\n",
    "                                                    df['is_spam'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size=300,\n",
    "                                   window=5,\n",
    "                                   min_count=1)\n",
    "\n",
    "# Test word vectors\n",
    "w2v_model.wv.most_similar('phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42c53052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Words: ['go', 'jurong', 'point', 'crazy', 'available', 'n', 'great', 'world', 'la', 'e'], Target Word: bugis\n",
      "Context Words: ['jurong', 'point', 'crazy', 'available', 'bugis', 'great', 'world', 'la', 'e', 'buffet'], Target Word: n\n",
      "Context Words: ['point', 'crazy', 'available', 'bugis', 'n', 'world', 'la', 'e', 'buffet', 'cine'], Target Word: great\n",
      "Context Words: ['crazy', 'available', 'bugis', 'n', 'great', 'la', 'e', 'buffet', 'cine', 'get'], Target Word: world\n",
      "Context Words: ['available', 'bugis', 'n', 'great', 'world', 'e', 'buffet', 'cine', 'get', 'amore'], Target Word: la\n",
      "Context Words: ['bugis', 'n', 'great', 'world', 'la', 'buffet', 'cine', 'get', 'amore', 'wat'], Target Word: e\n",
      "Context Words: ['n', 'great', 'world', 'la', 'e', 'cine', 'get', 'amore', 'wat', 'ok'], Target Word: buffet\n",
      "Context Words: ['great', 'world', 'la', 'e', 'buffet', 'get', 'amore', 'wat', 'ok', 'lar'], Target Word: cine\n",
      "Context Words: ['world', 'la', 'e', 'buffet', 'cine', 'amore', 'wat', 'ok', 'lar', 'joking'], Target Word: get\n",
      "Context Words: ['la', 'e', 'buffet', 'cine', 'get', 'wat', 'ok', 'lar', 'joking', 'wif'], Target Word: amore\n"
     ]
    }
   ],
   "source": [
    "# Continuous bag of words\n",
    "\n",
    "def generate_cbows(text, window_size):\n",
    "    \"\"\"Generate Continuous Bag of Words (CBOW) pairs from the given text. \n",
    "    text: preprocessed sequence of text\"\"\"\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "\n",
    "    # Create CBOW pairs with a given window size\n",
    "    cbows = []\n",
    "    for i, target_word in enumerate(words):\n",
    "        context_words = words[max(0, i - window_size):i] + words[i + 1:i + window_size + 1]\n",
    "        if len(context_words) == window_size * 2:\n",
    "            cbows.append((context_words, target_word))\n",
    "    return cbows\n",
    "\n",
    "# Create cbows\n",
    "cbows = generate_cbows(get_corpus(df), window_size=5)\n",
    "\n",
    "# Display the results\n",
    "for context_words, target_word in cbows[:10]:\n",
    "    print(f'Context Words: {context_words}, Target Word: {target_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c23a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word: str, unique_words: list):\n",
    "    try:\n",
    "        assert word in set(unique_words)\n",
    "    except AssertionError:\n",
    "        print(f\"Word '{word}' not found in unique_words.\")\n",
    "        return None\n",
    "    vector = np.zeros(len(unique_words))\n",
    "    index = unique_words.index(word)\n",
    "    vector[index] = 1\n",
    "    return tf.convert_to_tensor(vector)\n",
    "\n",
    "\n",
    "# Create one hot encodings\n",
    "\n",
    "unique_words = list(get_unique_words(df))\n",
    "one_hot_encodings = {\n",
    "    word: one_hot_encoding(word, unique_words) for word in unique_words\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ab02c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CBOW pairs to vector pairs\n",
    "cbow_vector_pairs = [([one_hot_encodings[word] for word in context_words], one_hot_encodings[target_word]) for context_words, target_word in cbows]\n",
    "\n",
    "# Sum the context vectors to get a single context vector\n",
    "cbow_vector_pairs = [(tf.reduce_sum(tf.stack(context_vectors), axis=0), target_vector) for context_vectors, target_vector in cbow_vector_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8b9c541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(7339,), dtype=float64, numpy=array([1., 1., 1., ..., 0., 0., 0.], shape=(7339,))>,\n",
       "  <tf.Tensor: shape=(7339,), dtype=float64, numpy=array([0., 0., 0., ..., 0., 0., 0.], shape=(7339,))>),\n",
       " (<tf.Tensor: shape=(7339,), dtype=float64, numpy=array([0., 1., 1., ..., 0., 0., 0.], shape=(7339,))>,\n",
       "  <tf.Tensor: shape=(7339,), dtype=float64, numpy=array([0., 0., 0., ..., 0., 0., 0.], shape=(7339,))>),\n",
       " (<tf.Tensor: shape=(7339,), dtype=float64, numpy=array([0., 0., 1., ..., 0., 0., 0.], shape=(7339,))>,\n",
       "  <tf.Tensor: shape=(7339,), dtype=float64, numpy=array([0., 0., 0., ..., 0., 0., 0.], shape=(7339,))>),\n",
       " (<tf.Tensor: shape=(7339,), dtype=float64, numpy=array([0., 0., 0., ..., 0., 0., 0.], shape=(7339,))>,\n",
       "  <tf.Tensor: shape=(7339,), dtype=float64, numpy=array([0., 0., 0., ..., 0., 0., 0.], shape=(7339,))>),\n",
       " (<tf.Tensor: shape=(7339,), dtype=float64, numpy=array([0., 0., 0., ..., 0., 0., 0.], shape=(7339,))>,\n",
       "  <tf.Tensor: shape=(7339,), dtype=float64, numpy=array([0., 0., 0., ..., 0., 0., 0.], shape=(7339,))>)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_vector_pairs[:5]  # Display first 5 CBOW vector pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a655d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveWord2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.weight1 = tf.keras.layers.Dense(embedding_dim, input_shape=(vocab_size,), activation='linear', name='embedding_layer')\n",
    "        self.weight2 = tf.keras.layers.Dense(vocab_size, input_shape=(embedding_dim,), activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.weight1(inputs)\n",
    "        x = self.weight2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m353/353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - accuracy: 0.0749 - loss: 7.4140 - val_accuracy: 0.0839 - val_loss: 7.1054\n",
      "Epoch 2/10\n",
      "\u001b[1m353/353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 54ms/step - accuracy: 0.0870 - loss: 6.4840 - val_accuracy: 0.0987 - val_loss: 6.8435\n",
      "Epoch 3/10\n",
      "\u001b[1m353/353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 53ms/step - accuracy: 0.1101 - loss: 5.7006 - val_accuracy: 0.1075 - val_loss: 6.6974\n",
      "Epoch 4/10\n",
      "\u001b[1m203/353\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 49ms/step - accuracy: 0.1542 - loss: 4.8738"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(unique_words)\n",
    "VECTOR_DIM = 300\n",
    "\n",
    "model = NaiveWord2Vec(VOCAB_SIZE, VECTOR_DIM)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'] # this is just word2vec\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x=tf.stack([pair[0] for pair in cbow_vector_pairs]),\n",
    "    y=tf.stack([pair[1] for pair in cbow_vector_pairs]),\n",
    "    epochs=10,\n",
    "    batch_size=128, \n",
    "    validation_split=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd1c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.2397558 ,  0.11211057,  0.04273067, ..., -0.17481793,\n",
       "          0.02447324,  0.19437271],\n",
       "        [ 0.01840967, -0.05884297, -0.02350952, ...,  0.09782509,\n",
       "          0.2138359 ,  0.09622125],\n",
       "        [ 0.24445544, -0.27639169, -0.15053558, ...,  0.18468523,\n",
       "         -0.23221882,  0.15553747],\n",
       "        ...,\n",
       "        [-0.26258704, -0.1626276 , -0.09193443, ..., -0.18885404,\n",
       "         -0.12884265,  0.18520987],\n",
       "        [-0.26023152,  0.02098822, -0.066674  , ..., -0.11843918,\n",
       "          0.09807865, -0.04264724],\n",
       "        [-0.08031567, -0.03805484, -0.03699608, ...,  0.05741283,\n",
       "         -0.00482738, -0.01303974]], shape=(7339, 300), dtype=float32),\n",
       " array([ 8.27972144e-02, -7.63747171e-02,  4.96348627e-02,  4.90374155e-02,\n",
       "        -6.90865740e-02, -2.99599897e-02, -1.32833719e-02,  4.51385416e-02,\n",
       "         7.77688846e-02,  3.01445033e-02,  5.25377020e-02, -1.37138650e-01,\n",
       "        -6.39462918e-02, -4.57772948e-02,  8.13158415e-03, -1.09567314e-01,\n",
       "         1.51452180e-02,  7.76919127e-02, -6.01652078e-02, -9.57761183e-02,\n",
       "        -4.26332317e-02, -3.51979360e-02,  6.20043911e-02,  1.95568539e-02,\n",
       "        -7.70446360e-02,  7.71659911e-02, -1.97256915e-02, -2.62638349e-02,\n",
       "         6.38747290e-02, -9.81343165e-02,  1.54132182e-02,  6.14180341e-02,\n",
       "        -4.00690874e-03, -1.36832893e-01,  1.09789938e-01,  3.51849310e-02,\n",
       "        -2.54109446e-02, -4.77320217e-02,  4.71434891e-02, -1.04544856e-01,\n",
       "        -8.77863094e-02,  1.34377599e-01, -2.40915008e-02,  1.15256965e-01,\n",
       "        -4.81117144e-02,  1.46208787e-02,  1.25781242e-02, -1.21678293e-01,\n",
       "         7.43000060e-02,  9.74632651e-02,  1.95395388e-02, -6.20959029e-02,\n",
       "         3.53603140e-02,  7.78613240e-02, -1.04107954e-01,  5.04085943e-02,\n",
       "         9.67814773e-02,  3.59396189e-02, -8.52269381e-02, -3.10600735e-02,\n",
       "        -1.11030517e-02, -3.94769236e-02, -7.76864365e-02, -1.33440597e-02,\n",
       "         5.22015914e-02, -3.81412767e-02,  1.15564495e-01, -6.58704340e-02,\n",
       "        -1.09134382e-02,  1.20323608e-02, -8.47647116e-02,  7.76154641e-03,\n",
       "        -6.50177002e-02,  1.28173232e-01, -3.51511277e-02,  9.21398401e-02,\n",
       "         4.20310311e-02,  3.47710252e-02,  4.74978297e-05,  3.67394462e-02,\n",
       "        -7.30598271e-02, -1.08092152e-01,  8.87527540e-02, -2.86993571e-02,\n",
       "         6.43210188e-02, -1.30968198e-01,  7.85541013e-02, -5.92712387e-02,\n",
       "         3.17243747e-02, -6.94996566e-02,  5.21272942e-02,  7.24326670e-02,\n",
       "         7.22527653e-02,  4.47875522e-02, -1.23579927e-01, -6.35386165e-03,\n",
       "        -9.76905823e-02,  1.93144046e-02,  8.21713880e-02, -3.83259952e-02,\n",
       "         2.59980839e-02,  1.68292709e-02,  1.02783509e-01,  1.73016097e-02,\n",
       "        -5.58289699e-02,  2.40167677e-02, -3.46308276e-02,  1.71195287e-02,\n",
       "        -2.86890045e-02,  9.99064278e-03,  6.43533468e-02, -5.33010578e-03,\n",
       "         4.77537178e-02,  4.28937934e-02, -9.43560153e-02,  5.24824187e-02,\n",
       "         1.70349870e-02, -1.00431249e-01,  1.49778314e-02, -1.41741540e-02,\n",
       "        -4.87109981e-02,  1.68846603e-02,  2.82782223e-02, -1.34077728e-01,\n",
       "         4.08175774e-02, -5.56206629e-02,  6.05678409e-02, -5.60462251e-02,\n",
       "         3.06668300e-02, -4.48959880e-02,  1.33239590e-02,  3.16820480e-02,\n",
       "        -4.39305138e-03,  1.69547312e-02,  1.53603464e-01,  7.52110258e-02,\n",
       "        -6.44682497e-02, -1.32513670e-02,  5.65371998e-02,  1.04396185e-02,\n",
       "         4.13710698e-02, -4.29900661e-02, -3.98929827e-02,  1.67200163e-01,\n",
       "         6.55900091e-02, -5.79815954e-02,  3.22682178e-03,  6.59146754e-04,\n",
       "         2.29513105e-02, -7.06300586e-02, -1.99586265e-02,  1.04001962e-01,\n",
       "        -9.52206831e-03, -8.94713029e-02, -9.41495672e-02,  6.98169693e-02,\n",
       "         1.14146667e-02, -4.75207111e-03, -8.73187184e-02,  4.80403844e-03,\n",
       "        -5.99362813e-02,  1.88543666e-02, -6.06663898e-02,  5.35951890e-02,\n",
       "         1.09671883e-01,  7.75309950e-02, -4.80151400e-02, -5.67820296e-02,\n",
       "        -8.90941769e-02,  9.91457514e-03,  6.34193234e-03, -4.13952246e-02,\n",
       "        -6.47209957e-02,  5.71806207e-02,  2.27239858e-02, -1.30782321e-01,\n",
       "         8.27632546e-02, -2.89124101e-02,  1.35547798e-02,  7.96817094e-02,\n",
       "        -8.34473521e-02,  2.01650970e-02, -8.87581706e-03, -5.77011481e-02,\n",
       "        -8.00225418e-03, -9.48458090e-02, -8.07711333e-02, -3.40957567e-02,\n",
       "        -2.88046300e-02,  3.35814022e-02, -4.43852134e-02,  1.52072043e-03,\n",
       "         4.47494164e-02, -7.59778395e-02, -9.71086919e-02, -8.35269317e-02,\n",
       "         2.00775135e-02, -3.30086960e-03, -3.53238843e-02, -1.33705400e-02,\n",
       "         8.43672976e-02,  2.10166834e-02, -1.07861623e-01, -1.21705547e-01,\n",
       "         6.01116419e-02, -1.33781810e-03,  3.22123170e-02, -1.05870314e-01,\n",
       "        -4.86189611e-02, -4.23667906e-03,  7.11173285e-04, -6.19780160e-02,\n",
       "        -1.03716865e-01,  5.90844788e-02, -2.37243879e-03,  5.38971759e-02,\n",
       "         1.43031906e-02,  4.41809259e-02,  9.32302773e-02, -4.81990762e-02,\n",
       "        -9.19808969e-02,  3.15836333e-02, -3.44950221e-02, -1.99418962e-02,\n",
       "         6.78004026e-02, -1.09762080e-01,  3.50917280e-02,  4.58347425e-02,\n",
       "        -3.81315164e-02,  1.09891877e-01, -4.94715683e-02, -2.24262979e-02,\n",
       "         3.80972996e-02,  1.68983098e-02, -6.37453049e-02, -4.87743551e-03,\n",
       "         9.30489507e-03, -1.25022391e-02,  3.61385792e-02,  1.16606953e-03,\n",
       "         6.08616434e-02,  5.13521880e-02,  5.06925769e-02,  1.21428281e-01,\n",
       "         3.12345363e-02, -3.14276740e-02, -3.14867087e-02,  3.82637754e-02,\n",
       "        -8.46205559e-03, -4.66973297e-02, -2.31478605e-02,  6.44231141e-02,\n",
       "         3.52247432e-02, -5.38724922e-02,  9.74941477e-02, -3.96789461e-02,\n",
       "        -9.85614434e-02,  1.16607189e-01,  1.70412231e-02, -1.11372747e-01,\n",
       "         3.68529074e-02,  2.94976961e-02,  2.62874961e-02, -9.98082012e-03,\n",
       "         7.00156167e-02, -3.45056653e-02, -3.65322120e-02,  5.39315157e-02,\n",
       "         7.36744981e-03,  8.22072402e-02, -2.57188268e-02,  6.88901097e-02,\n",
       "        -7.44875968e-02,  1.46157639e-02,  1.05478376e-01, -1.15662463e-01,\n",
       "         1.86753627e-02, -9.06001180e-02,  4.90245409e-02, -5.65673336e-02,\n",
       "         1.14170387e-02, -3.34563106e-02, -1.07293837e-01,  3.40834595e-02,\n",
       "        -3.92124690e-02, -5.14858356e-03,  4.01546247e-02, -6.33829385e-02,\n",
       "        -4.14377674e-02,  4.74541858e-02,  1.25072509e-01, -1.84056684e-02,\n",
       "        -1.39365047e-01,  6.23509921e-02,  8.16100985e-02,  4.59272526e-02,\n",
       "         4.37515415e-02, -9.94557701e-03, -7.79801607e-02,  7.33032227e-02],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('embedding_layer').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfae12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.15498084e-01, -6.93529770e-02, -3.11394818e-02,  4.49059606e-02,\n",
       "        6.07568435e-02,  1.79082140e-01, -2.33514830e-01,  4.60990258e-02,\n",
       "       -5.88371530e-02, -3.88602257e-01, -2.97492981e-01, -9.47781876e-02,\n",
       "        3.94366771e-01,  2.80486494e-01,  2.31879577e-01,  1.38568416e-01,\n",
       "       -3.26728761e-01, -1.42800305e-02,  8.98759216e-02,  2.35805407e-01,\n",
       "        4.07174349e-01, -2.40666598e-01,  3.34708810e-01,  4.08957526e-02,\n",
       "        6.88036010e-02, -2.35001482e-02,  2.65577231e-02, -3.41521442e-01,\n",
       "       -5.77514470e-02,  1.74865305e-01, -1.30803496e-01, -3.35410804e-01,\n",
       "        2.98039645e-01, -8.37064460e-02, -3.77371013e-01, -1.14328325e-01,\n",
       "        4.68625277e-02, -1.31423324e-01,  1.32083148e-01,  8.74082670e-02,\n",
       "        1.67630821e-01, -3.58226113e-02, -2.37804562e-01,  9.93765797e-03,\n",
       "       -8.21259841e-02, -2.11912200e-01,  8.19912739e-03, -2.35730708e-01,\n",
       "        1.07919194e-01,  2.39611566e-01, -2.06310749e-01,  2.53899582e-02,\n",
       "       -4.78892058e-01, -3.34016770e-01,  1.06313884e-01, -1.01926513e-01,\n",
       "        1.34986788e-01,  3.30330789e-01, -1.43523812e-01, -4.28875118e-01,\n",
       "        5.72256371e-02, -1.21885017e-01,  2.37162352e-01, -3.21993500e-01,\n",
       "       -8.53969529e-02, -5.50427139e-01, -2.19097048e-01,  1.23186685e-01,\n",
       "       -3.54475551e-03, -3.72636229e-01, -3.74811649e-01,  4.55820262e-02,\n",
       "       -1.15557775e-01,  9.68445539e-02, -1.05787508e-01, -2.25292712e-01,\n",
       "       -3.37811857e-01,  2.57534981e-01,  2.92555869e-01, -2.83313721e-01,\n",
       "        1.00335762e-01, -1.64312154e-01, -1.44623881e-02, -2.17676267e-01,\n",
       "        6.14562072e-02,  4.07879144e-01,  3.56213510e-01, -2.90174474e-04,\n",
       "        1.10655762e-01, -3.08608681e-01,  1.67466730e-01,  1.33577481e-01,\n",
       "        1.62812665e-01,  6.45114705e-02,  5.46124637e-01, -4.01752770e-01,\n",
       "       -2.47850418e-01, -1.60872906e-01, -3.42531800e-02, -6.75963014e-02,\n",
       "       -1.11579716e-01, -4.50484693e-01, -1.24929309e-01,  2.76987612e-01,\n",
       "        1.21910244e-01, -1.13444477e-01,  1.99645504e-01,  6.51361793e-02,\n",
       "       -8.05425644e-02,  4.41752791e-01, -6.14253758e-03,  2.20042959e-01,\n",
       "        1.47504255e-01,  2.47255296e-01,  5.51895611e-02,  1.08131552e-02,\n",
       "        9.44732353e-02, -2.70214498e-01, -2.40642503e-01, -1.81294575e-01,\n",
       "       -7.20410272e-02, -1.66728333e-01,  6.57888576e-02,  4.41317409e-02,\n",
       "       -8.39972422e-02,  1.03749856e-01,  4.43807781e-01,  1.73534855e-01,\n",
       "        2.19166234e-01,  4.21234444e-02,  3.18012267e-01, -1.36156544e-01,\n",
       "       -1.89089164e-01, -5.15110612e-01, -6.63996413e-02,  1.35961816e-01,\n",
       "        2.38182858e-01, -2.59107530e-01,  5.45158284e-03,  3.00506860e-01,\n",
       "       -1.23568222e-01, -4.96510081e-02, -1.33957878e-01, -1.92468822e-01,\n",
       "       -1.59032062e-01,  8.73187408e-02,  5.23845665e-02,  8.16820841e-03,\n",
       "        1.66191440e-02, -3.28072123e-02,  5.75611219e-02, -1.60849229e-01,\n",
       "       -2.82666385e-01, -1.70684516e-01, -1.80090562e-01, -5.88791743e-02,\n",
       "       -4.57435846e-02, -6.44750148e-02,  9.74111333e-02,  4.85626645e-02,\n",
       "       -8.70516375e-02,  3.79044772e-03,  1.03576295e-01,  2.00397402e-01,\n",
       "       -2.75346249e-01, -3.06824148e-01,  5.03214151e-02,  1.91462621e-01,\n",
       "       -1.87900931e-01,  7.35470802e-02, -1.80431455e-02,  7.29655921e-02,\n",
       "       -3.58579069e-01, -1.91850066e-01,  2.35704463e-02, -1.80810988e-01,\n",
       "       -7.93354362e-02, -2.54876852e-01, -2.00054228e-01,  1.46149322e-01,\n",
       "       -9.57645103e-02, -1.53339654e-01, -2.31431738e-01, -1.12068556e-01,\n",
       "       -3.96604873e-02, -5.36956966e-01,  1.13006972e-01, -7.20069325e-03,\n",
       "        2.88501889e-01, -6.40507713e-02, -7.90112093e-02,  1.69500902e-01,\n",
       "        1.17900528e-01,  1.23094872e-01, -1.38175324e-01,  4.48818095e-02,\n",
       "       -2.07832634e-01, -3.72019202e-01, -5.94838038e-02, -5.36431372e-02,\n",
       "       -8.42452049e-03,  2.99572796e-01, -2.94271678e-01,  1.77340489e-02,\n",
       "       -1.53303519e-02,  5.97559568e-03, -1.88388169e-01,  1.58953100e-01,\n",
       "        3.95954065e-02,  3.21123451e-02,  3.12915176e-01,  6.63841665e-02,\n",
       "        4.61741954e-01, -5.73508851e-02, -6.36007488e-01, -3.52122486e-02,\n",
       "        1.77115619e-01, -1.49544567e-01,  2.74426788e-01, -7.50343800e-02,\n",
       "        3.49929333e-02,  5.72855882e-02, -1.40126288e-01, -2.62559444e-01,\n",
       "       -1.11550555e-01, -5.25312543e-01,  1.76910028e-01,  8.31808969e-02,\n",
       "        4.14824963e-01,  5.33999428e-02, -1.60306424e-01, -2.56681386e-02,\n",
       "        4.69543897e-02,  1.51190490e-01, -1.00511715e-01,  1.72941238e-01,\n",
       "       -9.99457091e-02, -1.15424059e-01,  2.56693900e-01,  7.55046681e-02,\n",
       "       -1.31000504e-01,  2.59251714e-01,  2.01383099e-01, -2.45706797e-01,\n",
       "        2.90745616e-01, -4.02246416e-01,  1.25821099e-01, -1.10600546e-01,\n",
       "        4.25710022e-01,  7.72813782e-02, -4.48742732e-02, -1.62672788e-01,\n",
       "       -5.85880736e-03,  2.60610670e-01, -2.15313002e-01, -1.08121097e-01,\n",
       "       -7.67718852e-02,  1.16413705e-01,  3.03199500e-01, -5.45953512e-01,\n",
       "       -1.95551455e-01, -2.88547575e-01,  2.71862596e-01,  1.48838973e-02,\n",
       "        2.38811120e-01, -1.68510169e-01,  9.19365659e-02,  1.60121620e-01,\n",
       "       -9.38223079e-02, -2.77822226e-01, -5.85246645e-02,  1.23677067e-01,\n",
       "        1.18958682e-01, -4.94614393e-02,  2.70392164e-03, -2.50451893e-01,\n",
       "        4.28123385e-01, -1.85109317e-01,  1.57382518e-01, -2.24997327e-01,\n",
       "       -2.10312650e-01, -1.67136386e-01,  2.66949534e-01, -3.69667292e-01,\n",
       "       -1.19805530e-01, -2.24206313e-01, -5.08981682e-02, -5.60314301e-03,\n",
       "       -2.96089966e-02,  1.25736892e-01,  3.66920382e-01,  3.59969646e-01,\n",
       "       -1.41890436e-01,  4.25697938e-02,  1.37147039e-01, -2.38638356e-01,\n",
       "       -8.48929137e-02, -2.12436751e-01, -2.59983569e-01,  4.28678989e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get word embeddings\n",
    "embeddings = model.get_layer('embedding_layer').get_weights()[0]\n",
    "word_embeddings = {word: embeddings[idx] for idx, word in enumerate(unique_words)\n",
    "}\n",
    "word_embeddings['phone']  # Example embedding for the word 'phone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87333f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8240e278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.123276554)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(word_embeddings['phone'], word_embeddings['nokia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e0340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.0053015216)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(word_embeddings['phone'], word_embeddings['mom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fde5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.1789304)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(word_embeddings['mom'], word_embeddings['dad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b08c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
