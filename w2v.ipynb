{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b782c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-19 17:16:30.551033: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-19 17:16:30.967347: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-19 17:16:32.644937: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/tguyot/PersonalCode/spam-classifier/.venv/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tguyot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unidecode\n",
    "import re\n",
    "from spacy.cli import download\n",
    "import spacy\n",
    "import nltk\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a058b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_spam</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>go point crazy available boris n great world l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ok war joke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>free entry wily come win cup final st may text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>dun say early c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ah I think go live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>0</td>\n",
       "      <td>nd time try contact win pound prize claim easy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>1</td>\n",
       "      <td>I b go esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>1</td>\n",
       "      <td>pity mood suggestion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>1</td>\n",
       "      <td>guy itching I act like I interested buy someth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>1</td>\n",
       "      <td>roll true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5553 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      is_spam                                                sms\n",
       "0           1  go point crazy available boris n great world l...\n",
       "1           1                                        ok war joke\n",
       "2           0  free entry wily come win cup final st may text...\n",
       "3           1                        dun say early c already say\n",
       "4           1                   ah I think go live around though\n",
       "...       ...                                                ...\n",
       "5548        0  nd time try contact win pound prize claim easy...\n",
       "5549        1                           I b go esplanade fr home\n",
       "5550        1                               pity mood suggestion\n",
       "5551        1  guy itching I act like I interested buy someth...\n",
       "5552        1                                     roll true name\n",
       "\n",
       "[5553 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import cleaned preprocess csv\n",
    "df = pd.read_csv('cleaned_spam.csv').drop(columns=['Unnamed: 0'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70e6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'boris',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_corpus(df):\n",
    "    return ' '.join(' '.join(df.sms.tolist()).split())\n",
    "\n",
    "def get_unique_words(df):\n",
    "    corpus = get_corpus(df)\n",
    "    unique = set()\n",
    "    output = []\n",
    "    for word in corpus.split():\n",
    "        if word not in unique:\n",
    "            unique.add(word)\n",
    "            output.append(word)\n",
    "    return output\n",
    "\n",
    "def count_unique(df):\n",
    "    return len(get_unique_words(df))\n",
    "\n",
    "\n",
    "\n",
    "print(count_unique(df))\n",
    "\n",
    "get_unique_words(df)[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294dbd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WOrd2Vec Model\n",
    "class NaiveWord2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.weight1 = tf.keras.layers.Dense(embedding_dim, input_shape=(vocab_size,), activation='linear', name='embedding_layer')\n",
    "        self.weight2 = tf.keras.layers.Dense(vocab_size, input_shape=(embedding_dim,), activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.weight1(inputs)\n",
    "        x = self.weight2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6526975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('call', 0.9994032979011536),\n",
       " ('txt', 0.9993306994438171),\n",
       " ('free', 0.9992970824241638),\n",
       " ('go', 0.9992822408676147),\n",
       " ('win', 0.9992537498474121),\n",
       " ('make', 0.9992509484291077),\n",
       " ('phone', 0.9992388486862183),\n",
       " ('n', 0.9992247223854065),\n",
       " ('place', 0.9992219805717468),\n",
       " ('day', 0.9992161989212036),\n",
       " ('week', 0.9992114901542664),\n",
       " ('reply', 0.9992058277130127),\n",
       " ('one', 0.9992057085037231),\n",
       " ('get', 0.9991737604141235),\n",
       " ('give', 0.9991723299026489),\n",
       " ('text', 0.9991562962532043),\n",
       " ('take', 0.9991481900215149),\n",
       " ('number', 0.999146044254303),\n",
       " ('god', 0.999139666557312),\n",
       " ('b', 0.999131977558136)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with gensim first\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['sms'].apply(lambda x: x.split()),\n",
    "                                                    df['is_spam'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size=100,\n",
    "                                   window=2,\n",
    "                                   min_count=1)\n",
    "\n",
    "# Test word vectors -> lots of 0.999 cosine similarities\n",
    "w2v_model.wv.most_similar('mobile', topn=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c53052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous bag of words\n",
    "\n",
    "def generate_cbows(text, window_size):\n",
    "    \"\"\"Generate Continuous Bag of Words (CBOW) pairs from the given text. \n",
    "    text: preprocessed sequence of text\"\"\"\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "\n",
    "    # Create CBOW pairs with a given window size\n",
    "    cbows = []\n",
    "    for i, target_word in enumerate(words):\n",
    "        context_words = words[max(0, i - window_size):i] + words[i + 1:i + window_size + 1]\n",
    "        if len(context_words) == window_size * 2:\n",
    "            cbows.append((context_words, target_word))\n",
    "    return cbows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word: str, unique_words: list):\n",
    "    try:\n",
    "        assert word in set(unique_words)\n",
    "    except AssertionError:\n",
    "        print(f\"Word '{word}' not found in unique_words.\")\n",
    "        return None\n",
    "    vector = np.zeros(len(unique_words))\n",
    "    index = unique_words.index(word)\n",
    "    vector[index] = 1\n",
    "    return tf.convert_to_tensor(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab02c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cbow(df, window_size):\n",
    "    # Create Cbows non-encoded\n",
    "    cbows = generate_cbows(get_corpus(df), window_size=window_size)\n",
    "\n",
    "    # One-hot-encode words\n",
    "    unique_words = list(get_unique_words(df))\n",
    "    one_hot_encodings = {\n",
    "        word: one_hot_encoding(word, unique_words) for word in unique_words\n",
    "    }\n",
    "\n",
    "    # Convert CBOW pairs to vector pairs\n",
    "    cbow_vector_pairs = [([one_hot_encodings[word] for word in context_words], one_hot_encodings[target_word]) for context_words, target_word in cbows]\n",
    "\n",
    "    # Sum the context vectors to get a single context vector\n",
    "    cbow_vector_pairs = [(tf.reduce_sum(tf.stack(context_vectors), axis=0), target_vector) for context_vectors, target_vector in cbow_vector_pairs]\n",
    "\n",
    "    return cbow_vector_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tguyot/PersonalCode/spam-classifier/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - accuracy: 0.0704 - loss: 7.1284 - val_accuracy: 0.0842 - val_loss: 6.8380\n",
      "Epoch 2/15\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - accuracy: 0.1031 - loss: 6.2182 - val_accuracy: 0.1142 - val_loss: 6.5293\n",
      "Epoch 3/15\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - accuracy: 0.1653 - loss: 5.3102 - val_accuracy: 0.1383 - val_loss: 6.3778\n",
      "Epoch 4/15\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - accuracy: 0.2440 - loss: 4.3484 - val_accuracy: 0.1562 - val_loss: 6.3509\n",
      "Epoch 5/15\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - accuracy: 0.3388 - loss: 3.4446 - val_accuracy: 0.1716 - val_loss: 6.4410\n",
      "Epoch 6/15\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - accuracy: 0.4459 - loss: 2.6707 - val_accuracy: 0.1796 - val_loss: 6.6265\n",
      "Epoch 7/15\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - accuracy: 0.5576 - loss: 2.0599 - val_accuracy: 0.1813 - val_loss: 6.8693\n",
      "Epoch 8/15\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - accuracy: 0.6463 - loss: 1.6211 - val_accuracy: 0.1810 - val_loss: 7.1490\n",
      "Epoch 9/15\n",
      "\u001b[1m1180/1180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - accuracy: 0.7073 - loss: 1.3174 - val_accuracy: 0.1839 - val_loss: 7.4656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x73f23c1ef4d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = list(get_unique_words(df))\n",
    "VOCAB_SIZE = len(unique_words)\n",
    "VECTOR_DIM = 250\n",
    "WINDOW_SIZE = 2\n",
    "\n",
    "# best so far is VECTOR_DIM=200, WINDOW_SIZE=2, LR=0.01\n",
    "model = NaiveWord2Vec(VOCAB_SIZE, VECTOR_DIM)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Get cbow pairs\n",
    "cbow_vector_pairs = prepare_cbow(df, window_size=WINDOW_SIZE)\n",
    "\n",
    "model.fit(\n",
    "    x=tf.stack([pair[0] for pair in cbow_vector_pairs]),\n",
    "    y=tf.stack([pair[1] for pair in cbow_vector_pairs]),\n",
    "    epochs=15,\n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfae12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word embeddings\n",
    "embeddings = model.get_layer('embedding_layer').get_weights()[0]\n",
    "word_embeddings = {word: embeddings[idx] for idx, word in enumerate(unique_words)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87333f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Get ntop similar words\n",
    "def get_top_n_similar_words(target_word, word_embeddings, n=10):\n",
    "    target_vector = word_embeddings[target_word]\n",
    "    similarities = {}\n",
    "    for word, vector in word_embeddings.items():\n",
    "        if word != target_word:\n",
    "            similarities[word] = cosine_similarity(target_vector, vector)\n",
    "    # Sort by similarity\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_similarities[:n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8240e278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bonus', np.float32(0.54364353)),\n",
       " ('ppmpoboxbhambxe', np.float32(0.51361966)),\n",
       " ('hotmix', np.float32(0.44465983)),\n",
       " ('calloptoutj', np.float32(0.414103)),\n",
       " ('todayfrom', np.float32(0.4139428)),\n",
       " ('apsaward', np.float32(0.4122826)),\n",
       " ('tushin', np.float32(0.40710118)),\n",
       " ('freefone', np.float32(0.40527236)),\n",
       " ('polyphonic', np.float32(0.40508807)),\n",
       " ('dough', np.float32(0.39912173))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_similar_words('reward', word_embeddings, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0865d41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('magazine', np.float32(0.42165235)),\n",
       " ('mssuman', np.float32(0.39212197)),\n",
       " ('falconerf', np.float32(0.39149868)),\n",
       " ('priest', np.float32(0.36515862)),\n",
       " ('thuglyfe', np.float32(0.36444417)),\n",
       " ('careabout', np.float32(0.35936666)),\n",
       " ('mobstorequizppm', np.float32(0.35568982)),\n",
       " ('grahmbell', np.float32(0.352596)),\n",
       " ('parish', np.float32(0.35140562)),\n",
       " ('nitric', np.float32(0.3357464))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_similar_words('prize', word_embeddings, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e0340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-0.054397956)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(word_embeddings['phone'], word_embeddings['call']) # did not work as well as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fde5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.07887484)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(word_embeddings['text'], word_embeddings['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ecea0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('limited', np.float32(0.40236196)),\n",
       " ('landlineonly', np.float32(0.3769837)),\n",
       " ('backdoor', np.float32(0.37456143)),\n",
       " ('yard', np.float32(0.35390836)),\n",
       " ('textpod', np.float32(0.3136002)),\n",
       " ('mood', np.float32(0.31198224)),\n",
       " ('broad', np.float32(0.30957416)),\n",
       " ('sac', np.float32(0.30698884)),\n",
       " ('subscription', np.float32(0.30430934)),\n",
       " ('flag', np.float32(0.3036724)),\n",
       " ('george', np.float32(0.30110094)),\n",
       " ('q', np.float32(0.29500404)),\n",
       " ('rival', np.float32(0.29429808)),\n",
       " ('optoutdwv', np.float32(0.2913026)),\n",
       " ('calloptoutndx', np.float32(0.28420985)),\n",
       " ('teacoffee', np.float32(0.28236598)),\n",
       " ('esaplanade', np.float32(0.28191766)),\n",
       " ('logic', np.float32(0.2723593)),\n",
       " ('triple', np.float32(0.27178597)),\n",
       " ('nytecalpmsgp', np.float32(0.26528463))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get ntop similar words\n",
    "def get_top_n_similar_words(target_word, word_embeddings, n=10):\n",
    "    target_vector = word_embeddings[target_word]\n",
    "    similarities = {}\n",
    "    for word, vector in word_embeddings.items():\n",
    "        if word != target_word:\n",
    "            similarities[word] = cosine_similarity(target_vector, vector)\n",
    "    # Sort by similarity\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_similarities[:n]\n",
    "\n",
    "get_top_n_similar_words('free', word_embeddings, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01700f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bonus', np.float32(0.54364353)),\n",
       " ('ppmpoboxbhambxe', np.float32(0.51361966)),\n",
       " ('hotmix', np.float32(0.44465983)),\n",
       " ('calloptoutj', np.float32(0.414103)),\n",
       " ('todayfrom', np.float32(0.4139428)),\n",
       " ('apsaward', np.float32(0.4122826)),\n",
       " ('tushin', np.float32(0.40710118)),\n",
       " ('freefone', np.float32(0.40527236)),\n",
       " ('polyphonic', np.float32(0.40508807)),\n",
       " ('dough', np.float32(0.39912173)),\n",
       " ('quizclub', np.float32(0.39581454)),\n",
       " ('bannfwflyppm', np.float32(0.3954876)),\n",
       " ('neo', np.float32(0.3933182)),\n",
       " ('triple', np.float32(0.3862275)),\n",
       " ('vasa', np.float32(0.38414797)),\n",
       " ('subscriptngbpwk', np.float32(0.3800444)),\n",
       " ('identify', np.float32(0.3778111)),\n",
       " ('oral', np.float32(0.3687912)),\n",
       " ('videophone', np.float32(0.36657158)),\n",
       " ('smartcall', np.float32(0.35913646))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_similar_words('reward', word_embeddings, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08297729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('upgrade', np.float32(0.33815807)),\n",
       " ('rental', np.float32(0.31226462)),\n",
       " ('mrs', np.float32(0.30610892)),\n",
       " ('accessible', np.float32(0.2999165)),\n",
       " ('clip', np.float32(0.2913238)),\n",
       " ('getting', np.float32(0.28507674)),\n",
       " ('stanwood', np.float32(0.2780832)),\n",
       " ('worldgnun', np.float32(0.2761979)),\n",
       " ('nationwide', np.float32(0.26948124)),\n",
       " ('limbo', np.float32(0.2682088)),\n",
       " ('digital', np.float32(0.26754203)),\n",
       " ('tense', np.float32(0.26622087)),\n",
       " ('explosive', np.float32(0.26266536)),\n",
       " ('itplspls', np.float32(0.2603224)),\n",
       " ('refundedthis', np.float32(0.2581207)),\n",
       " ('youcarlo', np.float32(0.25473404)),\n",
       " ('anybody', np.float32(0.25124446)),\n",
       " ('engine', np.float32(0.24404491)),\n",
       " ('cela', np.float32(0.24353784)),\n",
       " ('hockey', np.float32(0.24111722))]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_similar_words('phone', word_embeddings, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb02dc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('multimedia', np.float32(0.4226807)),\n",
       " ('alertfrom', np.float32(0.4002517)),\n",
       " ('eckankar', np.float32(0.36506042)),\n",
       " ('personally', np.float32(0.35783258)),\n",
       " ('habbahw', np.float32(0.3513697)),\n",
       " ('messenger', np.float32(0.34484679)),\n",
       " ('multiply', np.float32(0.3417072)),\n",
       " ('recdthirtyeight', np.float32(0.3353375)),\n",
       " ('zindgi', np.float32(0.3325146)),\n",
       " ('storm', np.float32(0.31303832)),\n",
       " ('executive', np.float32(0.313)),\n",
       " ('responcewhat', np.float32(0.30844173)),\n",
       " ('intrude', np.float32(0.30188596)),\n",
       " ('noncomittal', np.float32(0.2933287)),\n",
       " ('dayswill', np.float32(0.28464863)),\n",
       " ('punch', np.float32(0.28203005)),\n",
       " ('logic', np.float32(0.28048545)),\n",
       " ('specify', np.float32(0.27729613)),\n",
       " ('border', np.float32(0.27591735)),\n",
       " ('anybody', np.float32(0.2737102))]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_similar_words('crazy', word_embeddings, n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
