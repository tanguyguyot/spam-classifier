{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e7e763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tguyot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unidecode\n",
    "import re\n",
    "from spacy.cli import download\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import contractions\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2782c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Pipeline is :\n",
    "#  1. Lowercase\n",
    "# 2. Remove accents\n",
    "# 3. Expand contractions\n",
    "# 4. Remove punctuation\n",
    "# 5. Remove numbers\n",
    "# 6. Correct typos\n",
    "# 7. Remove repeating characters\n",
    "# 8. Lemmatization\n",
    "# 9. Remove stopwords\n",
    "# 10. Remove empty lines\n",
    "df = pd.read_csv('spam.csv', encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a017e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_spam</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      is_spam                                                sms\n",
       "0           0  Go until jurong point, crazy.. Available only ...\n",
       "1           0                      Ok lar... Joking wif u oni...\n",
       "2           1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3           0  U dun say so early hor... U c already then say...\n",
       "4           0  Nah I don't think he goes to usf, he lives aro...\n",
       "...       ...                                                ...\n",
       "5567        1  This is the 2nd time we have tried 2 contact u...\n",
       "5568        0              Will Ì_ b going to esplanade fr home?\n",
       "5569        0  Pity, * was in mood for that. So...any other s...\n",
       "5570        0  The guy did some bitching but I acted like i'd...\n",
       "5571        0                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='latin1')\n",
    "\n",
    "# Clean up Unnamed columns and change column names for clarity\n",
    "df = df.assign(\n",
    "    is_spam=lambda x: 1\n",
    ")\n",
    "df.loc[df.v1 == 'ham', 'is_spam'] = 0\n",
    "df = df[['is_spam', 'v2']]\n",
    "df.columns = ['is_spam', 'sms']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9076f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial unique words: 15585\n"
     ]
    }
   ],
   "source": [
    "def get_corpus(df):\n",
    "    return ' '.join(' '.join(df.sms.tolist()).split())\n",
    "\n",
    "def count_unique(corpus):\n",
    "    return len(set(corpus.split()))\n",
    "\n",
    "corpus = get_corpus(df)\n",
    "initial = count_unique(corpus)\n",
    "print(f'Initial unique words: {initial}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08074ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after first cleaning (uniform capitalization, no accents, no punctuation, no numbers): 8610\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning\n",
    "\n",
    "# Uniform capitalization\n",
    "df_clean = df.copy()\n",
    "df_clean.sms = df_clean.sms.str.lower()\n",
    "\n",
    "# Replace accents\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: unidecode.unidecode(x))\n",
    "\n",
    "# Remove punctuation\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: re.sub(r'_', '', x))\n",
    "\n",
    "# Remove numbers\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: re.sub(r'\\d', '', x))\n",
    "\n",
    "print(f'Unique words after first cleaning (uniform capitalization, no accents, no punctuation, no numbers): {count_unique(get_corpus(df_clean))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acda18f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after typo correction: 6437\n"
     ]
    }
   ],
   "source": [
    "# Correct typos\n",
    "def correct_typos(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: correct_typos(x))\n",
    "print(f'Unique words after typo correction: {count_unique(get_corpus(df_clean))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717705b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after removing repeating characters: 6424\n"
     ]
    }
   ],
   "source": [
    "# Remove repeating characters\n",
    "\n",
    "def remove_repeating_characters(text):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: remove_repeating_characters(x))\n",
    "print(f'Unique words after removing repeating characters: {count_unique(get_corpus(df_clean))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef60490e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize\n",
    "\n",
    "# Need to download en_core_web_sm model if not already done\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d1dcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after lemmatization: 5421\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def to_lemma(text):\n",
    "    doc = nlp(str(text))\n",
    "    lemmatized_version = ''\n",
    "    for token in doc:\n",
    "        lemmatized_version += token.lemma_ + ' '\n",
    "    return lemmatized_version\n",
    "\n",
    "df_clean.sms = df_clean.sms.apply(\n",
    "    lambda x: to_lemma(x)\n",
    ")\n",
    "\n",
    "print(f'Unique words after lemmatization: {count_unique(get_corpus(df_clean))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c91e022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after expanding contractions: 5416\n"
     ]
    }
   ],
   "source": [
    "# Contractions fix (don't into do not etc...)\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: contractions.fix(x))\n",
    "\n",
    "print(f'Unique words after expanding contractions: {count_unique(get_corpus(df_clean))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8321eb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after removing stop words: 5277\n"
     ]
    }
   ],
   "source": [
    "# Remove \"stop words\": words not bringing information, in same high frequency everywhere\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: ' '.join([char for char in x.split() if char not in stop_words]))\n",
    "print(f'Unique words after removing stop words: {count_unique(get_corpus(df_clean))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c36e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove blank messages\n",
    "\n",
    "df_clean = df_clean.loc[df_clean.sms.map(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bea35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('cleaned_spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b39b978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final unique words after removing blank messages: 5277 for a corpus of 47189 words. Percentage of uniqueness: 11.18%\n"
     ]
    }
   ],
   "source": [
    "print(f'Final unique words after removing blank messages: {count_unique(get_corpus(df_clean))} for a corpus of {len(get_corpus(df_clean).split())} words. Percentage of uniqueness: {count_unique(get_corpus(df_clean))/len(get_corpus(df_clean).split()):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1d85c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We managed to divide by 3 (15587 -> 5277) the amount of unique words. This will help our model to generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4e725f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go point crazy available boris n great world la e buffet line get wat ok war joke free entry wily come win cup final st may text receive entry question txt rate apply dun say early c already say ah I ',\n",
       " 'think go live around though freemen hey darle week word back like fun still ok xx st cog send rev even brother like speak I treat I like aids patent per request selle selle minnaminunginte nurungu ves',\n",
       " 'ta set callertune caller press copy friend callertune winner value network customer select received prize reward claim call claim code valid hour mobile month r entitle update late colour mobile camer',\n",
       " 'a free call mobile update co free donna home soon I want talk stuff anymore tonight k give cry enough today six chance win cash pound txt cash send cost day day stand apply reply urgent win week free ',\n",
       " 'membership prize jackson txt word claim wwdbuknet lccltd pobox ldnwarw give search right word thank breathe I promise I take help grant fulfil promise wonderful blessing time I date sunday xxmobilemov',\n",
       " 'ieclub use credit click link next txt message click httpwap xxmobilemovieclubcomnqjkgighjjgcbl oh watch eh remember spell name yes I v naughty make I v wet fine thataos way feel thataos way get b engl',\n",
       " 'and v macedonia miss goalsteam news txt national team eg england trywale scotland txt poboxoxwwq seriously spell name sum go try month ha ha joke I pay first war da stock come aft I finish lunch I go ',\n",
       " 'sir smith finish lunch already ff alright way I meet soon force eat slice really hungry suck mark get worry know sick I turn penza always convincing catch bus try egg make tea eat moss leave dinner fe',\n",
       " 'el love back amp pack car ill let know room ah work I vaguely remember feel like wait still clear sure I sarcastic x want live yeah get v apologetic n fall action like spoil child get catch till go ba',\n",
       " 'dly cheer k tell I anything fear faint homework quick cuba thank subscription ringtone mobile charge apsmonth please confirm reply yes reply charge ok I go home look tiding I mug I xuhui go learn nd m']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a bunch of unique words\n",
    "\n",
    "corpus2 = [get_corpus(df_clean)[i:i+200] for i in range(0, 2000, 200)]\n",
    "corpus2\n",
    "\n",
    "# We notice that still plenty of words are typos, rare words, slang... We could try to use a lookup table, or a more advanced spell checker, but for now this will do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
