{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96e7e763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tguyot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unidecode\n",
    "import re\n",
    "from spacy.cli import download\n",
    "import spacy\n",
    "import nltk\n",
    "import contractions\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a017e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='latin1')\n",
    "\n",
    "# Clean up Unnamed columns and change column names for clarity\n",
    "df = df.assign(\n",
    "    is_spam=lambda x: 0\n",
    ")\n",
    "df.loc[df.v1 == 'ham', 'is_spam'] = 1\n",
    "df = df[['is_spam', 'v2']]\n",
    "df.columns = ['is_spam', 'sms']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9076f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial unique words: 15585\n"
     ]
    }
   ],
   "source": [
    "def get_corpus(df):\n",
    "    return ' '.join(' '.join(df.sms.tolist()).split())\n",
    "\n",
    "def count_unique(corpus):\n",
    "    return len(set(corpus.split()))\n",
    "\n",
    "corpus = get_corpus(df)\n",
    "initial = count_unique(corpus)\n",
    "print(f'Initial unique words: {initial}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9afdb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f08074ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after first cleaning (uniform capitalization, no accents, no punctuation, no numbers): 8610\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning\n",
    "\n",
    "    # Remplacer toutes les majuscules par des minuscules\n",
    "    # Retirer les accents (étape très utile pour les textes français)\n",
    "    # Retirer la ponctuation\n",
    "    # Retirer les nombres (seulement si ceux-ci n’apportent pas d’informations pour l’analyse qui suivra !!! Dans notre problème binaire, les nombres semblent apporter de l’information (comme les numéros de téléphone); on va les remplacer par “phonenumber”).\n",
    "    # Lemmatizer\n",
    "    # Enlever les “stop words”\n",
    "    # Supprimer les lignes vides \n",
    "\n",
    "# Uniform capitalization\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "df_clean.sms = df_clean.sms.str.lower()\n",
    "\n",
    "# Replace accents\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: unidecode.unidecode(x))\n",
    "\n",
    "# Remove punctuation\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: re.sub(r'_', '', x))\n",
    "\n",
    "# Remove numbers\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: re.sub(r'\\d', '', x))\n",
    "\n",
    "print(f'Unique words after first cleaning (uniform capitalization, no accents, no punctuation, no numbers): {count_unique(get_corpus(df_clean))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef60490e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Unique words after lemmatization: 7491\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize\n",
    "\n",
    "# Need to download en_core_web_sm model if not already done\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def to_lemma(text):\n",
    "    doc = nlp(str(text))\n",
    "    lemmatized_version = ''\n",
    "    for token in doc:\n",
    "        lemmatized_version += token.lemma_ + ' '\n",
    "    return lemmatized_version\n",
    "\n",
    "df_clean.sms = df_clean.sms.apply(\n",
    "    lambda x: to_lemma(x)\n",
    ")\n",
    "\n",
    "print(f'Unique words after lemmatization: {count_unique(get_corpus(df_clean))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c91e022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after expanding contractions: 7361\n"
     ]
    }
   ],
   "source": [
    "# Contractions fix (don't into do not etc...)\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: contractions.fix(x))\n",
    "\n",
    "print(f'Unique words after expanding contractions: {count_unique(get_corpus(df_clean))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8321eb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after removing stop words: 7339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99865/1450921243.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.sms = df_clean.sms.apply(lambda x: ' '.join([char for char in x.split() if char not in stop_words]))\n"
     ]
    }
   ],
   "source": [
    "# Remove \"stop words\": words not bringing information, in same high frequency everywhere\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_clean.sms = df_clean.sms.apply(lambda x: ' '.join([char for char in x.split() if char not in stop_words]))\n",
    "print(f'Unique words after removing stop words: {count_unique(get_corpus(df_clean))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63c36e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove blank messages\n",
    "\n",
    "df_clean = df_clean.loc[df_clean.sms.map(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2bea35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('cleaned_spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b39b978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final unique words after removing blank messages: 7339 for a corpus of 50152 words. Percentage of uniqueness: 14.63%\n"
     ]
    }
   ],
   "source": [
    "print(f'Final unique words after removing blank messages: {count_unique(get_corpus(df_clean))} for a corpus of {len(get_corpus(df_clean).split())} words. Percentage of uniqueness: {count_unique(get_corpus(df_clean))/len(get_corpus(df_clean).split()):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4dba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
